# -*- coding: utf-8 -*-
"""PairPlotsGeoChem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15PMi_IZxSI0cYXkM_qyf-O6_ExpSL3-7

# Pair Plots of Geochemical Data

## Import packages and the data.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from mpl_toolkits.mplot3d import Axes3D
import ipywidgets as widgets
from IPython.display import display
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Path to the Excel file
file_path = 'PairPlotData.xlsx'

# Read the Excel file
df = pd.read_excel(file_path, engine='openpyxl')

# Display the dataframe
df

# Define what parameter to use for differentiation
dp='Watershed'

# Quantity of unique Sample ID's
unique_sample_ids_count = df[dp].nunique()
print(f"Number of unique Sample IDs: {unique_sample_ids_count}")

unique_sample_ids_list = df[dp].unique().tolist()
print(f"Unique Sample IDs: {unique_sample_ids_list}")

"""## Pair Plots of the Raw Data
When you have a bunch of variables, it is often a good idea to explore them by generating "pair plots," which plot every possible pair of variables on a bivariate plot. I'm going to use the pairplot function in Seaborn, which has some cool features.  For instance, when a variable is paired with itself, you get some kind of histogram.  In this case, I'm making it do "kernel density estimates" which are histograms that have been smoothed to create an estimate of a probability distribution function. Also, I'm having it give different colors to the data from the African and Brazilian tourmalines so I can see which of the individual variables is good at separating the two categories.
"""

# Select only numeric columns, make a dataframe out of it, and then make another
# dataframe that also has a Provenance column.
numeric_df = df.select_dtypes(include=['number'])
df_New = numeric_df.copy()
df_New[dp] = df[dp]

# Create pair plots for the numeric columns
g = sns.pairplot(data=df_New, hue=dp, diag_kind='kde', palette='tab10', plot_kws={'alpha': 0.6})
g._legend.remove()
# Show the plot
fig_legend = plt.figure(figsize=(8, 4))
plt.legend(handles=g._legend_data.values(),  # Use pairplot's legend handles
           labels=g._legend_data.keys(),     # Use pairplot's legend labels
           loc='center', ncol=2, title=dp)
plt.axis('off')  # Hide axes in the legend figure
plt.tight_layout()
plt.show()

"""## Evaluating the Pair Plots
Here are the things I'm looking at in those pair plots.

1.   Some of the variables seem reasonably well correlated, but if there were more, then PCA would be a better bet. I'll try it, anyway. (PCA works best when there is a lot of correlation between variables.)
2.   If I had to pick the three best original variables to separate the categories, I would go with ZnO, CuO, and MgO.

## Principal Component Analysis
PCA is an "unsupervised" method of "dimensional reduction."  Essentially what you are doing is this. Think of the original variables as "axes" in n-dimensional space (n = # variables). You rotate these axes in such a way that the new axes (called principal components) take advantage of correlation between the original variables so you can explain more of the variance in the data in terms of fewer axes. That is, rotating the axes results in new axes that explain more of the variance in the data, so that if you throw out some of the new variables (dimensional reduction), you can do so without losing as much information.

One potential problem with PCA is that if some of your original variables have much larger values than others, the variance in those directions will dominate the choice of how to rotate the axes most effectively.  Therefore, you will typically "standardize" the different columns of data by z-scoring.  If you z-score a column of data, you are basically taking all the values, subtracting the mean value, and dividing by the standard deviation.  Therefore, every column of data will have a mean of zero and a standard deviation of 1, and the algorithm for rotating the axes will take all the variables equally into account.
"""

# Standardize the numeric data
scaler = StandardScaler()
numeric_scaled = scaler.fit_transform(numeric_df)

# Perform PCA
pca = PCA()
pca_scores = pca.fit_transform(numeric_scaled)

# Create a DataFrame for PCA scores
pca_scores_df = pd.DataFrame(pca_scores, columns=['PC ' + str(i+1) for i in range(pca_scores.shape[1])])

# Create a Series for explained variance
explained_variance = pd.Series(pca.explained_variance_, index=['PC ' + str(i+1) for i in range(pca_scores.shape[1])])

# Create a Series for explained variance ratio
explained_variance_ratio = pd.Series(pca.explained_variance_ratio_, index=['PC ' + str(i+1) for i in range(pca_scores.shape[1])])

# Create a DataFrame for the principal components matrix
components_df = pd.DataFrame(pca.components_, columns=['PC ' + str(i+1) for i in range(pca_scores.shape[1])], index=numeric_df.columns)

"""### PC "Scores"
Here we are going to display the "principal component scores," which are the same as the z-scored original data, but their coordinates are described in terms of the new axes (principal components). So same data, new axis system. Note that if I started with 18 variables, I get 18 PCs, because all I have done this point is rotate the axes.


"""

# Display the PC scores of the data.
pca_scores_df

"""### How Many PCs to Retain?
We started with 18 variables and we still have 18 variables, so our next step is to decide how many of the new variables (PCs) to retain, and how many to throw out. There are several ways to do this. However, note that the PCs are ranked as PC1, PC2, PC3, etc., in descending order based on how much of the data variance they explain. Therefore, the decision we make is how many of the PCs to retain, but it's always going to be PC1-PC4 or PC1-PC6, or whatever. That is, we retain the first few PCs and ignore the rest.

One way is called "the Rule of One." If you have z-scored your data, all the original variables will have a variance of 1.  (Variance is the standard deviation squared, so if the standard deviation is 1 for all the z-scored data, the variance will also be 1.) Therefore, you could retain all the PCs that explain variance greater than 1.  In this case, there are six.

The "explained_variance" we extracted from the PCA object above is where this information is tabulated.
"""

explained_variance

"""You could also look at the problem in terms of what percentage of the variance is explained by the retained PCs. Sometimes people say they want to retain enough PCs to explain at least 80% of the variance in the data, or something like that.  

The "explained variance ratio" we extracted shows the fraction of the total variance explained by each PC. E.g., PC1 explains ~24% of the total variance.  In this case, we would need about 9 PCs to explain 80% of the variance.  Not great.


"""

explained_variance_ratio

"""Some people do a "scree plot," which shows the cumulative variance explained as you retain more and more PCs. The idea is to see if there is an "elbow" in the plot where the total variance explained stops changing so drastically.  In the scree plot below, I don't see any elbow--another indication that PCA might not be the most effective way to handle this dataset.  """

plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance_ratio) + 1), np.cumsum(explained_variance_ratio), alpha=0.7, marker='o')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.xlabel('Principal Components')
plt.title('Scree Plot')
plt.xticks(range(1, len(explained_variance_ratio) + 1))
plt.show()

"""### Principal Component Equations
Above I also extracted the coefficients for the equations that describe the new variables (the PCs) as linear combination of the original (z-scored) data. For example, look at the "PC 1" column in the table below.  Note that there are 18 rows--one for each of the original variables.  I read that column to mean that if I want to calculate the PC 1 score of a data point, I will take the z-scored FeO concentration, multiply it by -0.155094, add 0.406593 * the MnO concentration, etc. In other words, the column is describing the new variables as linear combinations of the original variables.

When the data is z-scored, we can often get some insight about what the PCs actually mean.  For instance, I note that the largest coefficients for PC 1 are for CaO, K2O, and MnO, so I conclude that although PC 1 is a composite of all the original variables, it is most strongly influenced by these three.
"""

components_df

"""### Is the PCA Useful for Our Purpose?
Since PCA is unsupervised, we don't actually know if any of the important directions it finds in the data (PCs) are useful for our purposes.  

Here I do another set of pair plots, but for the PCs, rather than the original variables.  It looks to me like only PC2 might be useful for separating the African and Brazilian tourmalines.
"""

# Augment the pca_scores_df with the provenance
df_pca_scores = pca_scores_df.copy()
df_pca_scores['Provenance'] = df['Provenance']

# Create pair plots for the numeric columns
sns.pairplot(data=df_pca_scores, hue = 'Provenance', diag_kind='kde')

# Show the plot
plt.show()

"""### Plotting in PC Space
Here I'm going to plot the PC1-PC3 data in a 3D plot to see if I can see any good separation of the categories from any particular angle.  

I don't see very good separation, but I have to remember that the first 3 PCs only explain ~50% of the variance in the data.
"""

# Subset df where Provenance is 'Africa'
pca_scores_africa = df_pca_scores[df['Provenance'] == 'Africa']

# Subset df where Provenance is 'Brazil'
pca_scores_brazil = df_pca_scores[df['Provenance'] == 'Brazil']

def plot_3d(elev=0, azim=0):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(pca_scores_brazil["PC 1"], pca_scores_brazil["PC 2"], pca_scores_brazil["PC 3"], color='r', marker='o', label='Brazil')
    ax.scatter(pca_scores_africa["PC 1"], pca_scores_africa["PC 2"], pca_scores_africa["PC 3"], color='k', marker='o', label='Africa')
    ax.set_xlabel('PC 1')
    ax.set_ylabel('PC 2')
    ax.set_zlabel('PC 3')
    ax.view_init(elev=elev, azim=azim)
    ax.legend()
    plt.show()


widgets.interactive(plot_3d, elev=(-90, 90), azim=(0, 360))

"""### Blunt Force Dimensional Reduction?
Remember how we discovered from our first set of pair plots that if we had to pick three of the original variables that are best for separating the African and Brazilian gems, they would be ZnO, MgO, and CuO? Now I'm going to do a 3D plot with those variables as the axes to see how good the class separation is. In essence, we are doing "dimensional reduction" by simply throwing out all the variables that seem less relevant to the question at hand.

As you can see in the 3D plot, we get pretty decent separation of most of the data from the two classes.

"""

def plot_3d(elev=0, azim=0):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(df_brazil["ZnO"], df_brazil["MgO"], df_brazil["CuO"], color='r', marker='o', label='Brazil')
    ax.scatter(df_africa["ZnO"], df_africa["MgO"], df_africa["CuO"], color='k', marker='o', label='Africa')
    ax.set_xlabel('ZnO')
    ax.set_ylabel('MgO')
    ax.set_zlabel('CuO')
    ax.view_init(elev=elev, azim=azim)
    ax.legend()
    plt.show()


widgets.interactive(plot_3d, elev=(-90, 90), azim=(0, 360))

"""## Linear Discriminant Analysis
Linear discriminant analysis (LDA) is very similar to PCA in that it finds directions in the n-dimensional data space that are linear combinations of the original variables. However, LDA is a "supervised" method for classification. That is, while PCA finds directions in the data that correspond to maximum variance, there is no guarantee that those particular directions will be good for the task at hand--in this case, separating data from different classes. LDA models are trained on data for which the classes are already known, and finds directions in the data that are specifically good for separating the classes. Also, whereas the axes in PCA are rotated, and so remain orthogonal, this is not guaranteed in LDA.

In the following example, I once again use the z-scored data, because LDA can also be influenced by the relative magnitude of the numbers in the different variables.

I also separate the data into a training set and a test set.  The training set is used to calculate the linear discriminants, and then the test set is used to evaluate the model's accuracy. In this case I set it to retain 20% of the data in the test set.

Although you can do LDA on data that is separated into several classes, in this case we only have two classes.  Therefore, only one linear discriminant function will be generated.  (The number of discriminants is always the number of classes minus 1.)


"""

# Separate features and target
X = numeric_scaled  # predictor variables
y = df['Provenance']  # target variable

# Split data into training and testing sets (optional but recommended)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit the LDA model
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)

# Predict on the test set
y_pred = lda.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

"""### LDA Model Accuracy
Here is what I get out of the above report on the LDA model's effectiveness.

*   The accuracy score tells us that the categories for 95% of the data points in the test set were accurately predicted.
*   The precision scores tell us the ratio of true positive predictions to the total number of positive predictions (true positives and false positives).
  * It answers the question: "Of all the instances the model predicted as a certain class, how many were actually that class?"
  * High precision indicates a low false positive rate.
  * In this case, the precision is 95% for both the African and Brazilian classes.
*   Recall (also known as sensitivity or true positive rate) is the ratio of true positive predictions to the actual number of positives (true positives and false negatives).
  * It answers the question: "Of all the instances that were actually a certain class, how many did the model correctly identify?"
  * High recall indicates a low false negative rate.
  * In this case the recall was 97% for the African samples and 90% for the Brazilian. This means that it was slightly more probable to predict a false negative for the Brazilian gems.
*   The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both.
  * It is useful when you need a balance between precision and recall and is especially helpful if the class distribution is imbalanced.
  *   A high F1-score indicates both high precision and high recall.
*   Support is simply the number of true instances for each class in the test data.
  * It shows the distribution of classes in the actual data and helps assess how well the model performs across different classes, especially if the classes are imbalanced.
  * This shows that there were 20 Brazilian gems in the test set, but 39 African. For some kinds of analyses, a "balanced" dataset is preferable, meaning that you should have roughly equal numbers of data points in all the classes.

### Extracting Meaning from the LDA
In the table below, I show the coefficients for the linear equation that describes the discriminant direction. Since the data was z-scored, I can draw some inferences about which of the original variables are most important for discriminating between classes by looking at the absolute value of their corresponding coefficients.  In this case Na2O, CuO, ZnO, and MnO seem to be the most important.  Note there is some overlap with what we found eyeballing the pair plots above.
"""

# Create a Pandas series with the Linear Discriminant coefficients.
lda_coeffs = pd.Series(lda.coef_[0], index=numeric_df.columns, name='LD 1')
lda_coeffs

"""## Transforming the Data
Just as the PC "scores" above were simply the coordinates of the original z-scored data transformed into the new rotated axes, we can also calculate "scores" for the data in linear discriminant space.  

In this case, there is only one discriminant function, because there are only two categories. Therefore, I can calculate the LD1 scores and plot them as histograms.

Note that, in the histogram plot below, I could pick a value of the LD1 score and say that anything above that is from Brazil and anything below is from Africa. It wouldn't be 100% accurate, but as we found out above when we tried out the discriminant model on the test set, it was about 95% accurate, which isn't bad!
"""

# Transform the data to its lower-dimensional form
lda_transformed = lda.transform(numeric_scaled)
lda_df = pd.DataFrame(lda_transformed, columns=[f'LD {i+1}' for i in range(lda_transformed.shape[1])])

# To get the coefficients of the LDA model
df_lda_coeffs = pd.DataFrame(lda.coef_, index=['LD ' + str(i+1) for i in range(lda_df.shape[1])],
                             columns=numeric_df.columns)
df_lda_coeffs = df_lda_coeffs.T

print("LDA Scores")
lda_df

# Add a provenance column to lda_df.
lda_df['Provenance'] = df['Provenance']

# Group the dataframe by 'Provenance' and plot each group's 'LD1' values as a histogram
categories = lda_df['Provenance'].unique()
plt.figure(figsize=(10, 6))

# Iterate over each unique category in 'Provenance' and plot a histogram
for category in categories:
    subset = lda_df[lda_df['Provenance'] == category]
    plt.hist(subset['LD 1'], bins=20, alpha=0.5, label=category)

# Add labels and legend
plt.xlabel('LD 1 Score')
plt.ylabel('Frequency')
plt.legend(title='Provenance')
plt.show()

"""## REMEMBER!!!
Suppose you create a linear discriminant model like the one above and you want to apply it to new data as it comes in, to judge the provenance of more tourmaline samples. This creates a problem, because we fed z-scored data to the LDA function in scikit-learn.

To run the model on the new data, you would have to first apply THE SAME z-scoring that you did to the original data. That is, take your new data, subtract the mean of the data above, and divide by the standard deviation of the data above.  
"""

# Separate features and target
X = numeric_scaled  # predictor variables
y = df['Site']  # target variable

# Split data into training and testing sets (optional but recommended)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit the LDA model
lda2 = LinearDiscriminantAnalysis()
lda2.fit(X_train, y_train)

# Predict on the test set
y_pred2 = lda.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred2)
report = classification_report(y_test, y_pred2)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

# Extract the coefficients for the linear discriminants and put them into a
# dataframe called df_Coef.  The columns should be LD1, LD2, and LD3
coefficients = lda2.coef_

# Create column names as 'LD 1', 'LD 2', etc., based on the number of discriminants
column_names = [f'LD {i + 1}' for i in range(coefficients.shape[0])]

# Create the DataFrame with coefficients and column names
df_Coeff = pd.DataFrame(coefficients.T, columns=column_names)

df_Coeff

# Transform the data to get LDA scores
lda_scores2 = lda2.transform(numeric_scaled)

# Convert the LDA scores to a DataFrame for easier handling
df_scores2 = pd.DataFrame(lda_scores2, columns=['LD 1', 'LD 2', 'LD 3'])

# Add the 'Site' column for coloring
df_scores2['Site'] = df['Site']

def plot_3d(elev=0, azim=0):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    # Plot each site with a different color
    sites = df_scores2['Site'].unique()
    for site in sites:
        subset = df_scores2[df_scores2['Site'] == site]
        ax.scatter(subset['LD 1'], subset['LD 2'], subset['LD 3'], label=site)
    ax.set_xlabel('LD 1')
    ax.set_ylabel('LD 2')
    ax.set_zlabel('LD 3')
    ax.view_init(elev=elev, azim=azim)
    ax.legend()
    plt.show()

widgets.interactive(plot_3d, elev=(-90, 90), azim=(0, 360))